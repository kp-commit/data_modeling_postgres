# Sparkify
#### (Data Modeling on Postgres and Python ETL pipeline)
---

Sparkify company wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

The need is for data engineering to be done with Postgres database tables and optimize queries on song play analysis. A database schema and ETL pipeline been created for this analysis. Fact and dimension tables have been defined for a star schema with particular analytic focus on ETL pipeline that transfers data from all files in local directories into these tables in Postgres using Python Pandas and SQL.


## Datasets:
---

**Song Dataset**

The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset. 

```
song_data/A/B/C/TRABCEI128F424C983.json 
song_data/A/A/B/TRAABJL12903CDCF1A.json
```

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like:
```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
Log


```


**Log Dataset**

The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) with activity logs from a music streaming app based on specified configurations.

The log files in the dataset are partitioned by year and month.

For example, here are filepaths to two files in this dataset. 
```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```


And below is an example of what the data in a log file, 2018-11-12-events.json, looks like.

```
{"artist":"Blue October \/ Imogen Heap","auth":"Logged In","firstName":"Kaylee","gender":"F","itemInSession":7,"lastName":"Summers","length":241.3971,"level":"free","location":"Phoenix-Mesa-Scottsdale, AZ","method":"PUT","page":"NextSong","registration":1540344794796.0,"sessionId":139,"song":"Congratulations","status":200,"ts":1541107493796,"userAgent":"\"Mozilla\/5.0 (Windows NT 6.1; WOW64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/35.0.1916.153 Safari\/537.36\"","userId":"8"}
```


---

## Schema for Song Play Analysis
Using the song and log datasets, star schema optimized scheme for queries was created on song play analysis. This includes the following tables.


## Fact Table
**songplays** - records in log data associated with song plays i.e. records with page NextSong

```
songplay_id SERIAL PRIMARY KEY
start_time TIMESTAMP REFERENCES time(start_time)
user_id VARCHAR
level VARCHAR
song_id VARCHAR REFERENCES songs(song_id)
artist_id VARCHAR REFERENCES artists(artist_id)
session_id INT
location VARCHAR
user_agent VARCHAR
```

## Dimension Tables
**users** - users in the app

```
user_id VARCHAR PRIMARY KEY
first_name VARCHAR
last_name VARCHAR
gender VARCHAR
level VARCHAR)
```



**songs** - songs in music database
*song_id, title, artist_id, year, duration*

```
song_id VARCHAR PRIMARY KEY
title VARCHAR
artist_id VARCHAR
year INT
duration NUMERIC
```

**artists** - artists in music database
*artist_id, name, location, latitude, longitude*

```
artist_id VARCHAR PRIMARY KEY
name VARCHAR
location VARCHAR
latitude NUMERIC
longitude NUMERIC
```

**time** - timestamps of records in songplays broken down into specific units
*start_time, hour, day, week, month, year, weekday*

```
start_time TIMESTAMP PRIMARY KEY
hour INT
day INT
week INT
month INT
year INT
weekday VARCHAR
```

---

## Project Files:
1. **env.sh** - Script to setup PATH for execution environment
2. **data folder** - has all folders and subfolders with JSONs songs and log files
3. **create_tables.py** - drops and creates your tables. Run this file to reset your tables before
each time you run your ETL scripts.
4. **etl.py** - reads and processes files from song_data and log_data and loads them into tables.
5. **etl.ipynb** - Notebook contains detailed instructions on the ETL process for each of
the tables.
6. **sql_queries.py** - has all sql queries, to be imported into last three files above.
7. **test.ipynb** - Notebook, displays the first few rows of each table to you check database entries.

---


## Postgres Table creation steps:

1. Open New Terminal and Run env.sh script to set PATH to /home/workspace:

```
root@eb51801e9211:/home/workspace# ./env.sh
PATH updated
/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/workspace
root@eb51801e9211:/home/workspace#

```

2. Execute **create_tables.py** to drop existing and create tables

```
root@eb51801e9211:/home/workspace# ./create_tables.py
root@eb51801e9211:/home/workspace#

```

Verify Table Setup (*Optional*):

3. Open **test.ipynb** Notebook and run steps as listed to verify rows in each table.



## ETL Pipeline execution steps:

4. Execute **etl.py** to drop existing and create tables

```
root@eb51801e9211:/home/workspace# ./etl.py
71 files found in data/song_data
1/71 files processed.
2/71 files processed.
3/71 files processed.
4/71 files processed.
5/71 files processed.
6/71 files processed.
.
.
. (Ouput truncated)
.
.
26/30 files processed.
27/30 files processed.
28/30 files processed.
29/30 files processed.
30/30 files processed.
root@eb51801e9211:/home/workspace#

```

Verify ETL execution step-by-step (*Optional*):
5. Open **etl.ipynb** Notebook and run steps as listed inside to verify steps for each stage. Remember to close (restart) connection on test.ipynb and run execution of CREATE_TABLE.py once before running through each step.



Verify Table Setup (*Optional*):

6. Open **test.ipynb** Notebook and run steps as listed to **verify rows loaded successfully** in each table and **ETL completed.**


